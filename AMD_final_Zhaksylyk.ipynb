Delete old version of the report

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Downloading and Preparing the Dataset**\n",
        "\n",
        "This block downloads the Amazon Books Reviews dataset from Kaggle using the Kaggle API.  \n",
        "For security reasons, manual upload of the `kaggle.json` API key file is required at runtime.  \n",
        "After downloading, the dataset will be extracted and prepared for further analysis.\n"
      ],
      "metadata": {
        "id": "D7hHlE2UfAH1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "Mt_1YWQ5KYlK",
        "outputId": "8307d387-95f3-46e1-dae6-44bbf4d5f7ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload the kaggle.json file (Kaggle API credentials) when prompted below.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4dcbbf9f-8a21-4a11-b4c0-e2096b6374ae\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4dcbbf9f-8a21-4a11-b4c0-e2096b6374ae\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Kaggle API key successfully configured.\n",
            "Downloading the dataset from Kaggle...\n",
            "Dataset URL: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
            "License(s): CC0-1.0\n",
            "Downloading amazon-books-reviews.zip to /content\n",
            " 98% 1.04G/1.06G [00:04<00:00, 254MB/s]\n",
            "100% 1.06G/1.06G [00:05<00:00, 223MB/s]\n",
            "Dataset downloaded and extracted to the folder: amazon_reviews\n"
          ]
        }
      ],
      "source": [
        "# Install the Kaggle CLI tool (for dataset download)\n",
        "!pip install -q kaggle\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "# --- Step 1: Securely set up Kaggle credentials ---\n",
        "print(\"Upload the kaggle.json file (Kaggle API credentials) when prompted below.\")\n",
        "uploaded = files.upload()  # Prompted at runtime\n",
        "\n",
        "# Move kaggle.json to the default location for the Kaggle CLI\n",
        "kaggle_dir = os.path.expanduser(\"~/.kaggle\")\n",
        "os.makedirs(kaggle_dir, exist_ok=True)\n",
        "\n",
        "if \"kaggle.json\" in uploaded:\n",
        "    os.replace(\"kaggle.json\", os.path.join(kaggle_dir, \"kaggle.json\"))\n",
        "    os.chmod(os.path.join(kaggle_dir, \"kaggle.json\"), 0o600)\n",
        "    print(\"Kaggle API key successfully configured.\")\n",
        "else:\n",
        "    raise ValueError(\"A file named 'kaggle.json' is required.\")\n",
        "\n",
        "# --- Step 2: Download the dataset from Kaggle ---\n",
        "KAGGLE_DATASET = \"mohamedbakhet/amazon-books-reviews\"\n",
        "ZIP_FILE = \"amazon-books-reviews.zip\"\n",
        "\n",
        "print(\"Downloading the dataset from Kaggle...\")\n",
        "!kaggle datasets download -d $KAGGLE_DATASET\n",
        "\n",
        "# --- Step 3: Extract the dataset ---\n",
        "EXTRACT_DIR = \"amazon_reviews\"\n",
        "with zipfile.ZipFile(ZIP_FILE, 'r') as zip_ref:\n",
        "    zip_ref.extractall(EXTRACT_DIR)\n",
        "\n",
        "print(f\"Dataset downloaded and extracted to the folder: {EXTRACT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Loading and Inspecting the Dataset**\n",
        "\n",
        "This block loads the dataset into a pandas DataFrame and provides a quick overview of its structure and content, including the main columns and a few random sample reviews.\n"
      ],
      "metadata": {
        "id": "Knn2DX_8e_AZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Path to the extracted dataset\n",
        "file_path = \"amazon_reviews/Books_rating.csv\"\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(\n",
        "        f\"File not found at {file_path}. Please ensure the dataset has been downloaded and extracted.\"\n",
        "    )\n",
        "\n",
        "# Display the column names and dataset shape\n",
        "print(\"Columns in the dataset:\", list(df.columns))\n",
        "print(f\"Total number of rows: {df.shape[0]}\")\n",
        "print(f\"Total number of columns: {df.shape[1]}\")\n",
        "\n",
        "# Show a concise summary of the DataFrame (including column types and non-null counts)\n",
        "print(\"\\nDataFrame info:\")\n",
        "print(df.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X_dYcHFK9qF",
        "outputId": "0367b2ba-726a-418b-a5d6-d721480e7a95"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n",
            "Columns in the dataset: ['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
            "Total number of rows: 3000000\n",
            "Total number of columns: 10\n",
            "\n",
            "DataFrame info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000000 entries, 0 to 2999999\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Dtype  \n",
            "---  ------              -----  \n",
            " 0   Id                  object \n",
            " 1   Title               object \n",
            " 2   Price               float64\n",
            " 3   User_id             object \n",
            " 4   profileName         object \n",
            " 5   review/helpfulness  object \n",
            " 6   review/score        float64\n",
            " 7   review/time         int64  \n",
            " 8   review/summary      object \n",
            " 9   review/text         object \n",
            "dtypes: float64(2), int64(1), object(7)\n",
            "memory usage: 228.9+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Text Preprocessing**\n",
        "\n",
        "This block prepares the review texts for further analysis.  \n",
        "It decodes any HTML entities, converts text to lowercase, removes punctuation and stopwords, and extracts unique tokens from each review.  \n",
        "A sample of 1,000 reviews is used for demonstration.\n"
      ],
      "metadata": {
        "id": "h0o713x3gQbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import html\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and tokenizes review text:\n",
        "    - Decodes HTML entities (e.g., &quot; -> \")\n",
        "    - Converts text to lowercase\n",
        "    - Removes all characters except letters and spaces\n",
        "    - Removes English stopwords (common words with little meaning)\n",
        "    - Returns a set of unique tokens (words)\n",
        "    \"\"\"\n",
        "    text = html.unescape(text)  # Decode HTML entities like &quot;, &amp;, etc.\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Keep only letters and spaces\n",
        "    tokens = text.split()\n",
        "    tokens = [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n",
        "    return set(tokens)\n",
        "\n",
        "# Select a random sample of 1,000 non-empty reviews for demonstration\n",
        "sample_df = df[['review/text']].dropna().sample(1000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Apply preprocessing to each review and store the result as a new column\n",
        "sample_df['tokens'] = sample_df['review/text'].apply(preprocess_text)\n",
        "\n",
        "# Show a few example rows: original text and its token set\n",
        "print(sample_df[['review/text', 'tokens']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExZQgebWMRkZ",
        "outputId": "29c79a11-80a0-4a9a-9140-5944fb5d86d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         review/text  \\\n",
            "0  This book is a prime example of how horribly w...   \n",
            "1  Extremely disappointed by the SHORT length and...   \n",
            "2  I will openly admit that I have never seen the...   \n",
            "3  I loved this book so much that I selected it a...   \n",
            "4  Perhaps the best book on the market for passin...   \n",
            "\n",
            "                                              tokens  \n",
            "0  {characters, point, thinks, reviews, great, bi...  \n",
            "1  {cursory, stuff, length, better, extremely, bu...  \n",
            "2  {intimidated, reconnects, comedy, going, goess...  \n",
            "3  {loved, central, backdrop, journey, severity, ...  \n",
            "4  {price, runner, studying, valuable, keeper, bo...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a simple preprocessing approach: decoding HTML entities, lowercasing, removing non-letter characters, and splitting into unique tokens. This is fast and sufficient for massive data tasks, such as counting unique words or using a Bloom filter.  \n",
        "More advanced NLP tasks might require removing stopwords, stemming, or correcting misspelled words, but for large-scale analysis these steps are not critical and could even slow down the process.\n"
      ],
      "metadata": {
        "id": "Xjk3VjxFMn7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Finding Similar Reviews Using Jaccard Similarity**\n",
        "\n",
        "This block compares every possible pair of reviews in the sample using Jaccard similarity, which measures the overlap between the sets of unique words in each review.  \n",
        "Pairs with a similarity above a certain threshold (here, 0.5) are considered potentially similar.\n"
      ],
      "metadata": {
        "id": "FB_ntaH7g7BB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def jaccard_similarity(set1, set2):\n",
        "    \"\"\"\n",
        "    Computes the Jaccard similarity between two sets of tokens.\n",
        "    Jaccard similarity = size of intersection / size of union (range: 0 to 1)\n",
        "    \"\"\"\n",
        "    intersection = set1 & set2\n",
        "    union = set1 | set2\n",
        "    # If both sets are empty, define similarity as zero\n",
        "    return len(intersection) / len(union) if union else 0\n",
        "\n",
        "# Set the threshold above which reviews are considered \"similar\"\n",
        "SIMILARITY_THRESHOLD = 0.5\n",
        "similar_pairs = []\n",
        "\n",
        "# Check all unique pairs of reviews (this is O(N^2), works best on small samples)\n",
        "for i, j in combinations(range(len(sample_df)), 2):\n",
        "    sim = jaccard_similarity(sample_df.at[i, 'tokens'], sample_df.at[j, 'tokens'])\n",
        "    if sim >= SIMILARITY_THRESHOLD:\n",
        "        similar_pairs.append((i, j, sim))\n",
        "\n",
        "print(f\"Found {len(similar_pairs)} pairs with Jaccard similarity >= {SIMILARITY_THRESHOLD}\")\n",
        "\n",
        "# Print up to 5 of the most similar pairs, showing both texts and their similarity score\n",
        "for i, j, sim in similar_pairs[:5]:\n",
        "    print(f\"\\n--- Jaccard similarity: {sim:.2f} ---\")\n",
        "    print(\"Review 1:\", sample_df.at[i, 'review/text'])\n",
        "    print(\"Review 2:\", sample_df.at[j, 'review/text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ai85WRF1MWA9",
        "outputId": "832298f8-7455-416c-d8ba-c01a2a2b908e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 pairs with Jaccard similarity >= 0.5\n",
            "\n",
            "--- Jaccard similarity: 0.50 ---\n",
            "Review 1: A good book for ages ten on up to read. It is a book I would read again and again.\n",
            "Review 2: Good book to read - good for a book discussion group.\n",
            "\n",
            "--- Jaccard similarity: 1.00 ---\n",
            "Review 1: I'm sad to see the 87th Precinct series draw down to a close... This is (I believe) the first 87th Precinct novel released after Ed McBain's death... Fiddlers.Carella's group draws a case where a blind violin player was found shot twice in the head behind the restaurant where he worked. This quickly becomes more than \"just another murder\" when a sales rep for a cosmetic company is found dead in her home, same killing wound, and same murder weapon. Five murders in two weeks, all the same modus operandi, all the same gun, means they have a serial killer on their hands. But how do you tie together victims that have little in common except for the fact they're all over 50? All of the detectives of the 87th Precinct are tracking the murders, trying to find the common thread that will point to the killer. Meanwhile, the killer is on a mission to correct errors that only he knows and understands...As with all other 87th Precinct novels, I enjoyed this one quite a bit. I did find it a bit bittersweet, however. McBain passed away recently, so there's not much hope for too many more episodes. I've heard it rumored that he had one last novel \"in the can\" to be released upon his death. I'll have to keep an eye open for that, and for any other episodes that happened between this release and now. McBain is a true master of the police procedural, and his passing is a sad event for many of us. Fiddlers is a quick page turner that draws you into the life of the killer, as you try and figure out his motive and story.Classic McBain, and one to savor...\n",
            "Review 2: I'm sad to see the 87th Precinct series draw down to a close... This is (I believe) the first 87th Precinct novel released after Ed McBain's death... Fiddlers.Carella's group draws a case where a blind violin player was found shot twice in the head behind the restaurant where he worked. This quickly becomes more than \"just another murder\" when a sales rep for a cosmetic company is found dead in her home, same killing wound, and same murder weapon. Five murders in two weeks, all the same modus operandi, all the same gun, means they have a serial killer on their hands. But how do you tie together victims that have little in common except for the fact they're all over 50? All of the detectives of the 87th Precinct are tracking the murders, trying to find the common thread that will point to the killer. Meanwhile, the killer is on a mission to correct errors that only he knows and understands...As with all other 87th Precinct novels, I enjoyed this one quite a bit. I did find it a bit bittersweet, however. McBain passed away recently, so there's not much hope for too many more episodes. I've heard it rumored that he had one last novel \"in the can\" to be released upon his death. I'll have to keep an eye open for that, and for any other episodes that happened between this release and now. McBain is a true master of the police procedural, and his passing is a sad event for many of us. Fiddlers is a quick page turner that draws you into the life of the killer, as you try and figure out his motive and story.Classic McBain, and one to savor...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jaccard similarity is a straightforward and interpretable way to measure the overlap between two reviews' vocabularies.  \n",
        "While this brute-force approach works well on small samples and is easy to understand, it does not scale to large datasets due to its quadratic complexity.  \n",
        "For real-world large-scale data, more efficient methods such as MinHash + LSH are preferred.\n"
      ],
      "metadata": {
        "id": "zYXTqA-1hPbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Importing MinHash and LSH Tools**\n",
        "\n",
        "This block installs and imports the required library for scalable similarity search using MinHash and Locality Sensitive Hashing (LSH).  \n",
        "These methods allow finding similar reviews much more efficiently, especially when working with large datasets.\n"
      ],
      "metadata": {
        "id": "j2l7xIHghYH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasketch\n",
        "from datasketch import MinHash, MinHashLSH\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcCcWAS0M18e",
        "outputId": "5de7f1ba-0191-4c3f-aa5c-21753d3936cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/89.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Scalable Similarity Search with MinHash and LSH**\n",
        "\n",
        "MinHash + LSH is a scalable technique for detecting similar reviews in large datasets.\n",
        "By using compact MinHash signatures and efficient Locality Sensitive Hashing (LSH), we avoid comparing every possible review pair directly (which would be extremely slow for big data).\n",
        "You can control the sample size for testing or demonstration. If you set the sample size to `None`, the code will use the entire dataset—however, this may require more RAM and time than Google Colab allows.\n",
        "This approach demonstrates how the same pipeline can easily scale up to much larger datasets, as required by the project guidelines.\n"
      ],
      "metadata": {
        "id": "CrRnhOgNhqrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Set sample size here ---\n",
        "SAMPLE_SIZE = 1000  # Change to None for full dataset (may be very slow)\n",
        "\n",
        "# Create a new sample if needed (use all reviews if SAMPLE_SIZE=None)\n",
        "if SAMPLE_SIZE is not None:\n",
        "    sample_df = df[['review/text']].dropna().sample(SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
        "    print(f\" Using a random sample of {SAMPLE_SIZE} reviews.\")\n",
        "else:\n",
        "    sample_df = df[['review/text']].dropna().reset_index(drop=True)\n",
        "    print(f\" Using the full dataset ({len(sample_df)} reviews). This may take a long time.\")\n",
        "\n",
        "# Only preprocess if 'tokens' column does not exist yet\n",
        "if 'tokens' not in sample_df.columns:\n",
        "    sample_df['tokens'] = sample_df['review/text'].apply(preprocess_text)\n",
        "\n",
        "# --- MinHash + LSH for scalable similarity search ---\n",
        "from datasketch import MinHash, MinHashLSH\n",
        "\n",
        "def create_minhash(tokens, num_perm=128):\n",
        "    \"\"\"Create a MinHash signature from a set of tokens (words).\"\"\"\n",
        "    m = MinHash(num_perm=num_perm)\n",
        "    for token in tokens:\n",
        "        m.update(token.encode('utf8'))\n",
        "    return m\n",
        "\n",
        "# Compute MinHash for every review in the sample\n",
        "minhashes = [create_minhash(tokens) for tokens in sample_df['tokens']]\n",
        "\n",
        "# Create LSH index for fast similar-pair lookup\n",
        "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
        "for idx, mh in enumerate(minhashes):\n",
        "    lsh.insert(f\"doc_{idx}\", mh)\n",
        "\n",
        "# Find similar pairs by querying LSH for each review\n",
        "matches = []\n",
        "for idx, mh in enumerate(minhashes):\n",
        "    result = lsh.query(mh)\n",
        "    for r in result:\n",
        "        j = int(r.split(\"_\")[1])\n",
        "        # Only keep pairs where j > idx (no self-matches or duplicates)\n",
        "        if j > idx:\n",
        "            matches.append((idx, j))\n",
        "\n",
        "print(f\"Found {len(matches)} similar review pairs using MinHash + LSH.\")\n",
        "\n",
        "# Show up to 5 example pairs and their texts (trimmed to 300 chars)\n",
        "for i, j in matches[:5]:\n",
        "    print(\"\\n---\")\n",
        "    print(\"Review 1:\", sample_df.at[i, 'review/text'][:300], \"...\")\n",
        "    print(\"Review 2:\", sample_df.at[j, 'review/text'][:300], \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z96ZbI4OBlA",
        "outputId": "7187fa0c-df6e-4ecd-f7f4-f6a1321b19c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Using a random sample of 1000 reviews.\n",
            "Found 5 similar review pairs using MinHash + LSH.\n",
            "\n",
            "---\n",
            "Review 1: I've read this book already, and am reading it again for battle of the books at my school, and I thought it was going to be annoying, because I already know the plot, but it was just as exciting as the first time I read it. ...\n",
            "Review 2: This book was awsome it had a great combination of love and drama it so kept me wanting to read more and on top of reading a great book I crammed in some SAT words i would defenetly recomened this book ...\n",
            "\n",
            "---\n",
            "Review 1: I've read this book already, and am reading it again for battle of the books at my school, and I thought it was going to be annoying, because I already know the plot, but it was just as exciting as the first time I read it. ...\n",
            "Review 2: Unfortunately saw the movie before reading the book. Even though the movie had way more happening than the book. Excellent read though, plan on reading the others even though I've seen all the movies! ...\n",
            "\n",
            "---\n",
            "Review 1: Unfortunately saw the movie before reading the book. Even though the movie had way more happening than the book. Excellent read though, plan on reading the others even though I've seen all the movies! ...\n",
            "Review 2: This book was awsome it had a great combination of love and drama it so kept me wanting to read more and on top of reading a great book I crammed in some SAT words i would defenetly recomened this book ...\n",
            "\n",
            "---\n",
            "Review 1: A good book for ages ten on up to read. It is a book I would read again and again. ...\n",
            "Review 2: Good book to read - good for a book discussion group. ...\n",
            "\n",
            "---\n",
            "Review 1: I'm sad to see the 87th Precinct series draw down to a close... This is (I believe) the first 87th Precinct novel released after Ed McBain's death... Fiddlers.Carella's group draws a case where a blind violin player was found shot twice in the head behind the restaurant where he worked. This quickly ...\n",
            "Review 2: I'm sad to see the 87th Precinct series draw down to a close... This is (I believe) the first 87th Precinct novel released after Ed McBain's death... Fiddlers.Carella's group draws a case where a blind violin player was found shot twice in the head behind the restaurant where he worked. This quickly ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown above, MinHash + LSH efficiently finds similar review pairs even in a random sample of 1,000 reviews.\n",
        "By changing the `SAMPLE_SIZE` parameter, this same code can process much larger datasets, demonstrating its scalability.\n",
        "Note: On very large datasets (e.g., the entire Amazon Books file), processing time and memory usage depend on your hardware and environment limits."
      ],
      "metadata": {
        "id": "69M_jDs8ibhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Scalable Similarity Search with TF-IDF and Cosine Similarity**\n",
        "\n",
        "In this step, we use TF-IDF vectorization to represent each review as a numerical vector reflecting word importance, and cosine similarity to detect similar pairs.  \n",
        "Just like in the MinHash + LSH approach above, you can control the sample size to balance speed and scalability. By changing the sample size, this method can scale up to much larger datasets; however, calculating all pairwise similarities for the entire dataset requires a lot of memory and computation time.  \n",
        "For practical reasons, we demonstrate this method on a smaller sample, but the same technique can be applied to larger datasets (with memory considerations).\n"
      ],
      "metadata": {
        "id": "-j4ACoOdik2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import html\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# --- Set sample size here ---\n",
        "SAMPLE_SIZE = 1000  # Set to None for full dataset (may be very slow and use a lot of RAM!)\n",
        "\n",
        "# 1) Create a sample or use the entire dataset\n",
        "if SAMPLE_SIZE is not None:\n",
        "    sample_df = df[['review/text']].dropna().sample(SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
        "    print(f\"Using a random sample of {SAMPLE_SIZE} reviews.\")\n",
        "else:\n",
        "    sample_df = df[['review/text']].dropna().reset_index(drop=True)\n",
        "    print(f\"Using the full dataset ({len(sample_df)} reviews). This may require a lot of memory!\")\n",
        "\n",
        "# 2) Decode HTML entities before vectorizing\n",
        "#    This ensures that &quot;, &#34;, etc. become real quotation marks\n",
        "sample_df['clean_text'] = sample_df['review/text'].apply(html.unescape)\n",
        "\n",
        "# 3) Transform reviews into TF-IDF vectors (reflects word importance in each review)\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "tfidf_matrix = vectorizer.fit_transform(sample_df['clean_text'])\n",
        "\n",
        "# 4) Compute pairwise cosine similarities between all reviews\n",
        "cos_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# 5) Find all pairs with cosine similarity above the threshold (excluding self-matches and duplicates)\n",
        "THRESHOLD = 0.5\n",
        "pairs = []\n",
        "n = cos_sim.shape[0]\n",
        "for i in range(n):\n",
        "    for j in range(i + 1, n):\n",
        "        if cos_sim[i, j] >= THRESHOLD:\n",
        "            pairs.append((i, j, cos_sim[i, j]))\n",
        "\n",
        "print(f\"Found {len(pairs)} similar review pairs using TF-IDF + cosine similarity (threshold = {THRESHOLD})\")\n",
        "\n",
        "# 6) Show a few examples, decoding HTML entities for display\n",
        "for i, j, sim in pairs[:5]:\n",
        "    print(f\"\\n--- Cosine similarity: {sim:.2f} ---\")\n",
        "    review_i = html.unescape(sample_df.at[i, 'review/text'])\n",
        "    review_j = html.unescape(sample_df.at[j, 'review/text'])\n",
        "    print(\"Review 1:\", review_i[:300], \"...\")\n",
        "    print(\"Review 2:\", review_j[:300], \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykk2CtMVQrpU",
        "outputId": "d20c050f-4b02-47ee-ffb5-065c8ad6e00a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using a random sample of 1000 reviews.\n",
            "Found 3 similar review pairs using TF-IDF + cosine similarity (threshold = 0.5)\n",
            "\n",
            "--- Cosine similarity: 0.57 ---\n",
            "Review 1: Out of all the books I have read, The Fellowship of the Ring by J.R.R. Tolkien, the first book in the Lord of the Rings trilogy, would have to be my favorite book in three ways. To begin with, there is a tremendous amount of action in this book. For example, Frodo, the main character, and all of his ...\n",
            "Review 2: 'The Fellowship of The Ring', by JRR Tolkien, is the exciting first installment in 'The Lord of The Rings' trillogy. Building upon the story that was told in 'The Hobbit', this story tells the tale of the discovery by Gandalf the Gray, that the magic ring found by Bilbo Baggins on his journeys, is n ...\n",
            "\n",
            "--- Cosine similarity: 0.55 ---\n",
            "Review 1: First Jack Reacher book in the series, great plot, I never saw it coming. But, technically the events covered in The Enemy occur before this book begins. I did read The Enemy first and it explained a bit about how Reacher embarked on the life he leads in The Killing Floor. I have read all of the Jac ...\n",
            "Review 2: In his seventh Jack Reacher novel, Reacher once again finds himself in the wrong place at the wrong time to stay out of trouble. At night in Boston, he spots a man that he knows shouldn't be walking around alive get into a car and drive off. Back when Reacher was a military cop running investigation ...\n",
            "\n",
            "--- Cosine similarity: 1.00 ---\n",
            "Review 1: I'm sad to see the 87th Precinct series draw down to a close... This is (I believe) the first 87th Precinct novel released after Ed McBain's death... Fiddlers.Carella's group draws a case where a blind violin player was found shot twice in the head behind the restaurant where he worked. This quickly ...\n",
            "Review 2: I'm sad to see the 87th Precinct series draw down to a close... This is (I believe) the first 87th Precinct novel released after Ed McBain's death... Fiddlers.Carella's group draws a case where a blind violin player was found shot twice in the head behind the restaurant where he worked. This quickly ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown above, TF-IDF and cosine similarity efficiently detect both identical and thematically related reviews. For example, the method matched two different reviews of 'The Lord of the Rings', which share similar keywords and descriptions, as well as an exact duplicate in the 87th Precinct series.Plainly, our model uses clean_text (where entities are unescaped) for vectorization. However, the original reviews remain stored under review/text and may still contain entities like &quot;. That’s why we call html.unescape again when printing, so the output shows real quotation marks."
      ],
      "metadata": {
        "id": "sUk1kw2rjGkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "The original goal was to build a detector of similar book reviews—ideally by encoding the `review/text` column and using Jaccard similarity. We implemented three approaches:  \n",
        "- **Brute‐force Jaccard**, which is exact but only practical for a few thousand reviews.  \n",
        "- **TF-IDF + Cosine Similarity**, which adds semantic weighting but still requires O(N²) comparisons (suitable for up to ~10 000 reviews).  \n",
        "- **MinHash + LSH**, which approximates Jaccard overlap using compact sketches and scales efficiently to tens or hundreds of thousands of reviews.  \n"
      ],
      "metadata": {
        "id": "KmjIg8lUs9Cr"
      }
    }
  ]
}
